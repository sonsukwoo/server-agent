from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from contextlib import asynccontextmanager
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from src.agents.text_to_sql import run_text_to_sql, app as sql_app
from src.agents.mcp_clients.connector import postgres_client, qdrant_embeddings_client
from src.agents.middleware.input_guard import InputGuard
from config.settings import settings
import hashlib
from pathlib import Path
import logging
import json
from src.schema_listener import SchemaListener
from src.db.chat_store import chat_store
from src.api.chat import router as chat_router

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TEXT_TO_SQL")

class QueryRequest(BaseModel):
    agent: str  # "sql" ë˜ëŠ” "ubuntu"
    question: str

class QueryResponse(BaseModel):
    ok: bool
    agent: str
    data: dict | None = None
    error: str | None = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ìˆ˜ëª…ì£¼ê¸° í•¸ë“¤ëŸ¬"""
    schema_listener = None
    try:
        # ì±„íŒ… ê¸°ë¡ ìŠ¤í‚¤ë§ˆ ì´ˆê¸°í™” (í•­ìƒ)
        await chat_store.ensure_chat_schema()
    except Exception as e:
        logger.error("CHAT_STORE: ensure schema failed: %s", e)

    if settings.enable_schema_sync:
        try:
            # 1. ì´ˆê¸° ë™ê¸°í™” (1íšŒ)
            await sync_schema_embeddings_mcp()

            # 2. ë¦¬ìŠ¤ë„ˆ ì‹œì‘ (Background)
            schema_listener = SchemaListener(callback=sync_schema_embeddings_mcp)
            await schema_listener.start()

        except Exception as e:
            logger.error("SCHEMA_EMBED: MCP sync/listener setup failed: %s", e)
            
    yield
    
    # ì¢…ë£Œ ì²˜ë¦¬
    if schema_listener:
        await schema_listener.stop()


app = FastAPI(title="Server Agent API", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# Router ë“±ë¡
app.include_router(chat_router)

@app.get("/")
async def root():
    return {"message": "Server Agent API is running"}


async def sync_schema_embeddings_mcp() -> None:
    """DB ìŠ¤í‚¤ë§ˆë¥¼ ì½ì–´ Qdrant MCP ì„ë² ë”© ì„œë²„ë¡œ ì—…ì„œíŠ¸"""
    logger.info("ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ë™ê¸°í™” ì‹œì‘")

    excluded_schemas = tuple(
        s.strip() for s in settings.schema_exclude_namespaces.split(",") if s.strip()
    )
    # SQL ì¸ì ì…˜ ë°©ì§€ë¥¼ ìœ„í•´ íŒŒë¼ë¯¸í„° ë°”ì¸ë”©ì€ ì–´ë µì§€ë§Œ, settings ê°’ì€ ì‹ ë¢°í•œë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜
    # ì•ˆì „í•˜ê²Œ í¬ë§·íŒ…. ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ ë³€í™˜.
    excluded_str = ", ".join(f"'{s}'" for s in excluded_schemas)
    
    tables_sql = f"""
    SELECT
      n.nspname AS schema,
      c.relname AS table_name,
      obj_description(c.oid, 'pg_class') AS description
    FROM pg_class c
    JOIN pg_namespace n ON n.oid = c.relnamespace
    WHERE c.relkind IN ('r','p','v')
      AND n.nspname NOT IN ({excluded_str})
    ORDER BY n.nspname, c.relname;
    """

    columns_sql = f"""
    SELECT
      n.nspname AS schema,
      c.relname AS table_name,
      a.attname AS column_name,
      format_type(a.atttypid, a.atttypmod) AS data_type,
      col_description(a.attrelid, a.attnum) AS description
    FROM pg_attribute a
    JOIN pg_class c ON a.attrelid = c.oid
    JOIN pg_namespace n ON c.relnamespace = n.oid
    WHERE a.attnum > 0
      AND NOT a.attisdropped
      AND c.relkind IN ('r','p','v')
      AND n.nspname NOT IN ({excluded_str})
    ORDER BY n.nspname, c.relname, a.attnum;
    """

    async with postgres_client() as client:
        tables_raw = await client.call_tool("execute_sql", {"query": tables_sql})
        columns_raw = await client.call_tool("execute_sql", {"query": columns_sql})

    tables = json.loads(tables_raw) if tables_raw else []
    columns = json.loads(columns_raw) if columns_raw else []

    column_map: dict[tuple[str, str], list[dict]] = {}
    for col in columns:
        key = (col.get("schema", ""), col.get("table_name", ""))
        column_map.setdefault(key, []).append({
            "name": col.get("column_name", ""),
            "type": col.get("data_type", ""),
            "description": col.get("description") or "",
            "role": "dimension",
            "category": "general",
            "visible_to_llm": True,
        })

    docs = []
    for t in tables:
        schema = t.get("schema", "")
        table_name = t.get("table_name", "")
        columns_list = column_map.get((schema, table_name), [])
        docs.append({
            "doc_type": "table",
            "schema": schema,
            "table_name": table_name,
            "description": t.get("description") or "",
            "primary_time_col": _infer_primary_time(columns_list),
            "join_keys": _infer_join_keys(columns_list),
            "columns": columns_list,
            "source": "db_schema",
        })

    if not docs:
        logger.info("ìŠ¤í‚¤ë§ˆ í…Œì´ë¸” ì—†ìŒ: ì„ë² ë”© ìŠ¤í‚µ")
        return

    # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸/ìƒì„± (ì—†ìœ¼ë©´ ìƒì„±ë˜ë¯€ë¡œ ì—…ì„œíŠ¸ ê°•ì œ)
    collection_created = False
    async with qdrant_embeddings_client() as qclient:
        ensure_msg = await qclient.call_tool("ensure_collection", {"vector_size": 1536})
        if isinstance(ensure_msg, str) and ("ìƒì„±" in ensure_msg or "created" in ensure_msg.lower()):
            collection_created = True

        # schema hash ë¹„êµ (ë³€ê²½ ì—†ê³  ì»¬ë ‰ì…˜ë„ ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
        schema_hash = _schema_hash(docs)
        stored_hash = _read_hash_file()
        if stored_hash == schema_hash and not collection_created:
            logger.info("ìŠ¤í‚¤ë§ˆ ë³€ê²½ ì—†ìŒ: ì„ë² ë”© ìŠ¤í‚µ")
            return

        await qclient.call_tool("upsert_schema", {"docs": docs})

    _write_hash_file(schema_hash)
    logger.info("ìŠ¤í‚¤ë§ˆ ì„ë² ë”© ì™„ë£Œ: í…Œì´ë¸” %sê°œ", len(docs))


def _schema_hash(docs: list[dict]) -> str:
    payload = []
    for doc in docs:
        payload.append(
            {
                "doc_type": doc.get("doc_type"),
                "schema": doc.get("schema"),
                "table_name": doc.get("table_name"),
                "description": doc.get("description"),
                "columns": [
                    {
                        "name": c.get("name"),
                        "type": c.get("type"),
                        "description": c.get("description"),
                    }
                    for c in doc.get("columns", [])
                ],
            }
        )
    canonical = json.dumps(payload, ensure_ascii=True, sort_keys=True)
    return hashlib.sha256(canonical.encode("utf-8")).hexdigest()


def _infer_primary_time(columns: list[dict]) -> str | None:
    for col in columns:
        if col.get("name") == "ts":
            return "ts"
    for col in columns:
        if col.get("name") in {"time", "timestamp", "created_at"}:
            return col.get("name")
    return None


def _infer_join_keys(columns: list[dict]) -> list[str]:
    keys: list[str] = []
    for col in columns:
        name = (col.get("name") or "").lower()
        if name == "ts":
            keys.append("ts")
        if name.endswith("_id") or name in {"host", "host_id", "container_id", "mount", "interface"}:
            keys.append(col.get("name") or "")
    seen: set[str] = set()
    deduped = []
    for k in keys:
        if k and k not in seen:
            seen.add(k)
            deduped.append(k)
    return deduped


def _read_hash_file() -> str | None:
    path = Path(settings.schema_hash_file)
    try:
        if path.exists():
            return path.read_text(encoding="utf-8").strip() or None
    except Exception as e:
        logger.warning("SCHEMA_EMBED: hash read failed: %s", e)
    return None


def _write_hash_file(schema_hash: str) -> None:
    path = Path(settings.schema_hash_file)
    try:
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(schema_hash, encoding="utf-8")
    except Exception as e:
        logger.warning("SCHEMA_EMBED: hash write failed: %s", e)


@app.post("/query")
async def query(body: QueryRequest):
    """ìì—°ì–´ ì§ˆë¬¸ì„ ë°›ì•„ì„œ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)"""
    agent_type = body.agent.lower().strip()
    question = body.question.strip()

    # 1. ì…ë ¥ ê²€ì¦
    if not question:
        raise HTTPException(status_code=400, detail="ì§ˆë¬¸ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
    
    is_valid, error = InputGuard.validate(question)
    if not is_valid:
        raise HTTPException(status_code=400, detail=error)

    # ë…¸ë“œ ì´ë¦„ê³¼ ìƒíƒœ ë©”ì‹œì§€ ë§¤í•‘
    node_messages = {
        "parse_request": "ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ì¤‘...",
        "validate_request": "ì§ˆë¬¸ ìœ íš¨ì„± ê²€ì¦ ì¤‘...",
        "retrieve_tables": "ê´€ë ¨ í…Œì´ë¸” ê²€ìƒ‰ ì¤‘...",
        "select_tables": "ì¡°íšŒì— í•„ìš”í•œ í…Œì´ë¸” ì„ íƒ ì¤‘...",
        "generate_sql": "SQL ì¿¼ë¦¬ ìƒì„± ì¤‘...",
        "guard_sql": "SQL ì•ˆì „ì„± ê²€ì‚¬ ì¤‘...",
        "execute_sql": "ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒ ì¤‘...",
        "normalize_result": "ì¡°íšŒ ê²°ê³¼ ì •ë¦¬ ì¤‘...",
        "validate_llm": "ê²°ê³¼ ì •í™•ì„± ê²€ì¦ ì¤‘...",
        "expand_tables": "í…Œì´ë¸” í™•ì¥ ê²€ìƒ‰ ì¤‘...",
        "generate_report": "ìµœì¢… ë³´ê³ ì„œ ì‘ì„± ì¤‘...",
    }

    async def event_generator():
        if agent_type == "sql":
            initial_state = {
                "user_question": question,
                "sql_retry_count": 0,
                "table_expand_count": 0,
                "validation_retry_count": 0,
                "total_loops": 0,
                "verdict": "OK",
                "result_status": "unknown",
                "failed_queries": [],
                "table_expand_attempted": False,
                "table_expand_failed": False,
                "table_expand_reason": None,
            }
            
            last_reason = ""
            current_retry = 0
            try:
                # LangGraph astream í˜¸ì¶œ
                async for event in sql_app.astream(initial_state):
                    for node_name, output in event.items():
                        # ìƒíƒœ ì—…ë°ì´íŠ¸ ì¶”ì 
                        if "validation_reason" in output:
                            last_reason = output["validation_reason"]
                        
                        # ì¬ì‹œë„ íšŸìˆ˜ ì—…ë°ì´íŠ¸
                        node_retry = output.get("sql_retry_count") or output.get("validation_retry_count")
                        if node_retry is not None:
                            current_retry = node_retry
                        
                        # íŠ¹ì • ë…¸ë“œê°€ ì‹œì‘ë˜ê±°ë‚˜ ì™„ë£Œë  ë•Œ ìƒíƒœ ë©”ì‹œì§€ ì „ì†¡
                        status_msg = node_messages.get(node_name)
                        if status_msg:
                            # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: generate_sqlì—ì„œ ì¬ì‹œë„ ì¤‘ì¸ ê²½ìš° ìƒì„¸ ì‚¬ìœ  í¬í•¨
                            if node_name == "generate_sql" and current_retry > 0:
                                if last_reason:
                                    # ì‚¬ìœ ë¥¼ ì§§ê²Œ ìš”ì•½í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ í‘œì‹œ (ì‚¬ìš©ì ìš”ì²­: "ë¡œê·¸ì— ë‚˜ì˜¤ë“¯ ë‚˜ì˜¤ê²Œ")
                                    status_msg = f"í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ SQL ì¬ì‘ì„± ì¤‘ (ì‚¬ìœ : {last_reason}) [ì¬ì‹œë„ {current_retry}]"
                                else:
                                    status_msg = f"ì˜¤ë¥˜ ë³µêµ¬ ë° SQL ì¬ì‘ì„± ì¤‘... [ì¬ì‹œë„ {current_retry}]"
                            
                            yield f"data: {json.dumps({'type': 'status', 'message': status_msg, 'node': node_name}, ensure_ascii=False)}\n\n"
                        
                        # íˆ´ ì‚¬ìš© ë¡œê·¸ê°€ ìˆìœ¼ë©´ ì´ë²¤íŠ¸ ì „ì†¡
                        tool_usage = output.get("last_tool_usage")
                        if tool_usage:
                            tool_msg = f"ğŸ› ï¸ [íˆ´ ì‚¬ìš©] {tool_usage}"
                            yield f"data: {json.dumps({'type': 'status', 'message': tool_msg, 'node': node_name}, ensure_ascii=False)}\n\n"
                        
                        # ë§ˆì§€ë§‰ ê²°ê³¼ì¸ ê²½ìš° ì „ì²´ ë°ì´í„° ì „ì†¡
                        if node_name == "generate_report":
                            final_data = {
                                "ok": True,
                                "agent": "sql",
                                "data": {
                                    "report": output.get("report", ""),
                                    "suggested_actions": output.get("suggested_actions", []),
                                    "raw": output
                                }
                            }
                            yield f"data: {json.dumps({'type': 'result', 'payload': final_data}, ensure_ascii=False)}\n\n"
            except Exception as e:
                logger.error("STREAM_ERROR: %s", e)
                yield f"data: {json.dumps({'type': 'error', 'message': str(e)}, ensure_ascii=False)}\n\n"
        elif agent_type == "ubuntu":
            yield f"data: {json.dumps({'type': 'error', 'message': 'Ubuntu ì—ì´ì „íŠ¸ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'}, ensure_ascii=False)}\n\n"
        else:
            yield f"data: {json.dumps({'type': 'error', 'message': f'ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—ì´ì „íŠ¸ íƒ€ì…ì…ë‹ˆë‹¤: {agent_type}'}, ensure_ascii=False)}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/resource-summary")
async def get_resource_summary():
    """ops_metrics.v_resource_summary ë·°ì—ì„œ ìµœê·¼ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ìš”ì•½ì„ ê°€ì ¸ì˜´"""
    sql = "SELECT * FROM ops_metrics.v_resource_summary ORDER BY \"ë°°ì¹˜ ID\" DESC LIMIT 1"
    try:
        async with postgres_client() as client:
            result_raw = await client.call_tool("execute_sql", {"query": sql})
            if not result_raw:
                return {}
            
            try:
                result = json.loads(result_raw)
            except json.JSONDecodeError as je:
                return {}

            if result and isinstance(result, list):
                return result[0]
            return {}
    except Exception as e:
        return {}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
